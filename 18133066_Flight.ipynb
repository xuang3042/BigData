{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fcb82d",
   "metadata": {},
   "source": [
    "### 18133066 - Trần Thị Lệ Xuân"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06242d34",
   "metadata": {},
   "source": [
    "# Model Pipeline - SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57dce1",
   "metadata": {},
   "source": [
    "## 1. Giới thiệu về SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506ff2c",
   "metadata": {},
   "source": [
    "SparkSQL là một module của Apache spark cho phép thực hiện các biến đổi với dữ liệu có cấu trúc và các tính toán dựa trên cụm xử lý. Spark cho phép tính toán data phân tán bằng việc chia dữ liệu thành nhiều phần nhỏ và thực hiện tính toán song song từng phần trên nhiều node server khác nhau. Chính vì thế tốc độ xử lý của spark rất nhanh và phù hợp với những dữ liệu kích thước lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8ced8",
   "metadata": {},
   "source": [
    "Mỗi một tính toán trên Spark sẽ được thực hiện trên một cluster được quản lý bởi một máy chủ (master). Máy chủ sẽ thực hiện 2 nhiệm vụ chính đó là: Phân chia vùng dữ liệu tính toán về các nodes được phụ trách bởi các máy con (wokers) để thực hiện xử lý dữ liệu. Sau khi các máy con hoàn thành nhiệm vụ, master sẽ tổng hợp lại các kết quả từ các máy con và trả về client kết quả cuối cùng. Cơ chế này có thể được mô tả thông qua sơ đồ bên dưới."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20558b0",
   "metadata": {},
   "source": [
    "## 2. Cơ chế hoạt động của spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff1608",
   "metadata": {},
   "source": [
    "Bước đầu tiên để thiết lập spark là tạo ra 1 cụm xử lý (cluster) trên một máy chủ. Cụm xử lý này được kết nối tới rất nhiều nodes khác nhau. Máy chủ (master) sẽ làm nhiệm vụ: phân chia dữ liệu và quản lý tính toán trên các máy con. máy chủ sẽ kết nối đến các máy con (slaves) trong cụm bằng các session. Máy chủ sẽ gửi dữ liệu và yêu cầu tính toán để máy con thực thi. Sau khi có kết quả máy con có nhiệm vụ trả về máy chủ. Máy chủ tổng hợp tất cả các tính toán trên máy con để tính ra kết quả cuối cùng.\n",
    "\n",
    "Trong bài này do mới làm quen với spark nên mình sẽ khởi tạo một cluster trên local. Thay vì kết nối tới những máy khác, các tính toán sẽ được thực hiện chỉ trên server local thông qua một giả lập cụm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce375df4",
   "metadata": {},
   "source": [
    "### 2.1. Khởi tạo một connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d5d11",
   "metadata": {},
   "source": [
    "Để khởi tạo một connection: tạo ra một instance của SparkContext class. Class này sẽ có một vài tham số yêu cầu chúng ta phải khai báo để xác định các thuộc tính của cụm mà chúng ta muốn connect tới.\n",
    "\n",
    "Những tham số này có thể được cấu hình thông qua constructor SparkConf().\n",
    "\n",
    "Một vài tham số quan trọng:\n",
    "\n",
    "+ Master: url connect tới master.\n",
    "\n",
    "+ AppName: Tên ứng dụng.\n",
    "\n",
    "+ SparkHome: Đường dẫn nơi spark được cài đặt trên các nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724f61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark \n",
    "import findspark\n",
    "\n",
    "# Initialize and provide path\n",
    "findspark.init(\"D:\\spark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd3dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0b896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc have not yet created!\n",
      "<SparkContext master=local appName=First app>\n",
      "2.4.8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('sc have not yet created!')\n",
    "    \n",
    "sc = SparkContext(master = \"local\", appName = \"First app\")\n",
    "# Check spark context\n",
    "print(sc)\n",
    "# Check spark context version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13ccece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=First app>\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82014715",
   "metadata": {},
   "source": [
    "### 2.2. Khởi tạo một Session trong SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e58b1",
   "metadata": {},
   "source": [
    "Khởi tạo session thông qua hàm SparkSession\n",
    "\n",
    "Hàm getOrCreate() của session cũng tương tự như context để tránh trường hợp phát sinh lỗi khi session đã tồn tại và đang hoạt động nhưng được khởi tạo lại.\n",
    "\n",
    "Thuộc tính catalog trong 1 Session sẽ liệt kê toàn bộ các dữ liệu có bên trong 1 cluster. Trong đó để xem toàn bộ các bảng đang có trong cluster chúng ta sử dụng hàm .listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbde95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000016DF9170C50>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "# Print my_spark session\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09a474b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all table exist in spark sesion\n",
    "my_spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f18bc",
   "metadata": {},
   "source": [
    "#### Load dữ liệu tập file \"flight.csv\" đã tải về"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4029d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|    1|  1|          4|     AS|           98|     N407AS|           ANC|                SEA|               0005|          2354|            -11|      21|      0015|           205|         194|     169|    1448|     0404|      4|             0430|        0408|          -22|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|         2336|     N3KUAA|           LAX|                PBI|               0010|          0002|             -8|      12|      0014|           280|         279|     263|    2330|     0737|      4|             0750|        0741|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     US|          840|     N171US|           SFO|                CLT|               0020|          0018|             -2|      16|      0034|           286|         293|     266|    2296|     0800|     11|             0806|        0811|            5|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|          258|     N3HYAA|           LAX|                MIA|               0020|          0015|             -5|      15|      0030|           285|         281|     258|    2342|     0748|      8|             0805|        0756|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AS|          135|     N527AS|           SEA|                ANC|               0025|          0024|             -1|      11|      0035|           235|         215|     199|    1448|     0254|      5|             0320|        0259|          -21|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = my_spark.read.csv('flights.csv', header = True)\n",
    "# show fligths top 5\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa214687",
   "metadata": {},
   "source": [
    "Để kiểm tra các schema có trong một table chúng ta sử dụng hàm printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cc90379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: string (nullable = true)\n",
      " |-- MONTH: string (nullable = true)\n",
      " |-- DAY: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: string (nullable = true)\n",
      " |-- DEPARTURE_DELAY: string (nullable = true)\n",
      " |-- TAXI_OUT: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- SCHEDULED_TIME: string (nullable = true)\n",
      " |-- ELAPSED_TIME: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      " |-- TAXI_IN: string (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: string (nullable = true)\n",
      " |-- ARRIVAL_TIME: string (nullable = true)\n",
      " |-- ARRIVAL_DELAY: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- AIRLINE_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20a6ac",
   "metadata": {},
   "source": [
    "### 2.3. Thêm một spark DataFrame lưu trữ local vào một catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09afdf",
   "metadata": {},
   "source": [
    "Tuy nhiên lúc này flights vẫn chỉ là một spark DataFrame chưa có trong catalog của của cluster. Sử dụng hàm listTable() liệt kê danh sách bảng ta thu được 1 list rỗng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccf12dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# list all table exist in spark session\n",
    "print(my_spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d93ca",
   "metadata": {},
   "source": [
    "Lý do là bởi khi được đọc từ hàm read.csv() thì dữ liệu chỉ được lưu trữ ở Local dưới dạng một spark DataFrame. Để đưa dự liệu từ local lên cluster chúng ta cần save nó dưới dạng một temporary table thông một trong những lệnh bên dưới:\n",
    "\n",
    "+ .createTempView(): là một phương thức của spark DataFrame trong đó tham số duy nhất được truyền vào là tên bảng mà bạn muốn lưu trữ dưới dạng temporary table. Bảng được tạo ra là tạm thời và chỉ có thể được truy cập từ session được sử dụng để tạo ra spark DataFrame.\n",
    "\n",
    "+ .createOrReplaceTempView(): Có tác dụng hoàn toàn giống như .createTempView() nhưng nó sẽ update lại temporary table nếu nó đã tồn tại hoặc tạo mới nếu chưa tồn tại trước dây. Tránh trường hợp duplicate dữ liệu.\n",
    "\n",
    "Ngoài ra chúng ta còn sử dụng:\n",
    "\n",
    "+ .createDataFrame(): Tạo một spark DataFrame từ một pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d07b02",
   "metadata": {},
   "source": [
    "#### Khởi tạo một temporary table với tên là flights_temp cho bảng flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2e84e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temporary table on catalog of local data frame flights as new temporary table flights_temp on catalog\n",
    "flights.createOrReplaceTempView('flights_temp')\n",
    "# check list all table available on catalog\n",
    "my_spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8bd24",
   "metadata": {},
   "source": [
    "### 2.4. Các lệnh biến đổi dữ liệu của spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe71c8b",
   "metadata": {},
   "source": [
    "Thông thường sẽ gồm các lệnh chính như tạo trường, update trường, xóa trường. Các lệnh bên dưới đều là các thuộc tính của spark DataFrame.\n",
    "\n",
    ".withColumn(“newColumnName”, formular): Thêm một trường mới vào một bảng sẵn có. Gồm 2 tham số chính, tham số thứ nhất là tên trường mới, tham số thứ 2 là công thức cập nhật tên trường. Lưu ý rằng spark DataFrame là một dạng dữ liệu immutable (không thể modified được). Do đó ta không thể inplace update (như các hàm fillna() hoặc replace() của pandas dataframe) mà cần phải gán giá trị trả về vào chính tên bảng ban đầu để cập nhật trường mới.\n",
    "\n",
    "Chẳng hạn bên dưới ta sẽ thêm 1 trường mới là HOUR_ARR được tính dựa trên AIR_TIME/60 (qui từ phút ra h) của bảng flights như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886b3b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: string (nullable = true)\n",
      " |-- MONTH: string (nullable = true)\n",
      " |-- DAY: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: string (nullable = true)\n",
      " |-- DEPARTURE_DELAY: string (nullable = true)\n",
      " |-- TAXI_OUT: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- SCHEDULED_TIME: string (nullable = true)\n",
      " |-- ELAPSED_TIME: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      " |-- TAXI_IN: string (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: string (nullable = true)\n",
      " |-- ARRIVAL_TIME: string (nullable = true)\n",
      " |-- ARRIVAL_DELAY: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- AIRLINE_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      " |-- HOUR_ARR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = flights.withColumn('HOUR_ARR', flights.AIR_TIME/60)\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9678b",
   "metadata": {},
   "source": [
    "+ .withColumnRenamed(“oldColumnName”, “newColumnName”): Đổi tên của một column name trong pandas DataFrame.\n",
    "\n",
    "+ .select(“column1”, “column2”, … , “columnt”, formular): Lựa chọn danh sách các trường trong spark DataFrame thông qua các tên column được truyền vào đưới dạng string và tạo ra một trường mới thông qua formular. Lưu ý để đặt tên cho trường mới ứng với formular chúng ta sẽ cần sử dụng hàm formula.alias(\"columnName\").\n",
    "\n",
    "Bên dưới chúng ta sẽ tạo ra trường avg_speed tính vận tốc trung bình của các máy bay bằng cách lấy khoảng cách (DISTANCE) chi cho thời gian bay (HOUR_ARR) group by theo mã máy bay (TAIL_NUMBER) bằng lệnh select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6462f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      "\n",
      "+--------------+-------------------+-----------+-----------------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|TAIL_NUMBER|        avg_speed|\n",
      "+--------------+-------------------+-----------+-----------------+\n",
      "|           ANC|                SEA|     N407AS|514.0828402366864|\n",
      "|           LAX|                PBI|     N3KUAA|531.5589353612166|\n",
      "|           SFO|                CLT|     N171US|517.8947368421052|\n",
      "|           LAX|                MIA|     N3HYAA|544.6511627906978|\n",
      "|           SEA|                ANC|     N527AS|436.5829145728643|\n",
      "+--------------+-------------------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_speed = flights.select(\"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\", \"TAIL_NUMBER\", (flights.DISTANCE/flights.HOUR_ARR).alias(\"avg_speed\"))\n",
    "avg_speed.printSchema()\n",
    "avg_speed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383501e",
   "metadata": {},
   "source": [
    ".selectExpr(“column1”, “column2”, … , “columnt”, “formularExpr”): Hoàn toàn tương tự như .select() nhưng tham số formular được thay thế bằng chuỗi string biểu diễn công thức như trong câu lệnh SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e814dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      "\n",
      "+--------------+-------------------+-----------+-----------------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|TAIL_NUMBER|        avg_speed|\n",
      "+--------------+-------------------+-----------+-----------------+\n",
      "|           ANC|                SEA|     N407AS|514.0828402366864|\n",
      "|           LAX|                PBI|     N3KUAA|531.5589353612166|\n",
      "|           SFO|                CLT|     N171US|517.8947368421052|\n",
      "|           LAX|                MIA|     N3HYAA|544.6511627906978|\n",
      "|           SEA|                ANC|     N527AS|436.5829145728643|\n",
      "+--------------+-------------------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_speed_exp = flights.selectExpr(\"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\", \"TAIL_NUMBER\", \"(DISTANCE/HOUR_ARR) AS avg_speed\")\n",
    "avg_speed_exp.printSchema()\n",
    "avg_speed_exp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7a5b5",
   "metadata": {},
   "source": [
    ".filter(condition): Lọc một bảng theo một điều kiện nào đó. Condition có thể làm một string expression biểu diễn công thức lọc hoặc một công thức giữa các trường trong spark DataFrame. Lưu ý Condition phải trả về một trường dạng Boolean type.\n",
    "\n",
    "#### Chẳng hạn chúng ta muốn lọc những chuyến bay xuất phát từ sân bay SEA và điểm đến là ANC ta có thể sử dụng filter như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aef60d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+------------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|          HOUR_ARR|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+------------------+\n",
      "|2015|    1|  1|          4|     AS|          135|     N527AS|           SEA|                ANC|               0025|          0024|             -1|      11|      0035|           235|         215|     199|    1448|     0254|      5|             0320|        0259|          -21|       0|        0|               null|            null|          null|         null|               null|         null| 3.316666666666667|\n",
      "|2015|    1|  1|          4|     AS|           81|     N577AS|           SEA|                ANC|               0600|          0557|             -3|      25|      0622|           234|         224|     195|    1448|     0837|      4|             0854|        0841|          -13|       0|        0|               null|            null|          null|         null|               null|         null|              3.25|\n",
      "|2015|    1|  1|          4|     AS|           83|     N532AS|           SEA|                ANC|               0800|          0802|              2|      14|      0816|           235|         211|     194|    1448|     1030|      3|             1055|        1033|          -22|       0|        0|               null|            null|          null|         null|               null|         null|3.2333333333333334|\n",
      "|2015|    1|  1|          4|     AS|          111|     N570AS|           SEA|                ANC|               0905|          0859|             -6|      30|      0929|           230|         226|     193|    1448|     1142|      3|             1155|        1145|          -10|       0|        0|               null|            null|          null|         null|               null|         null| 3.216666666666667|\n",
      "|2015|    1|  1|          4|     AS|           85|     N764AS|           SEA|                ANC|               1020|          1020|              0|      19|      1039|           223|         220|     198|    1448|     1257|      3|             1303|        1300|           -3|       0|        0|               null|            null|          null|         null|               null|         null|               3.3|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_SEA_ANC = flights.filter(\"ORIGIN_AIRPORT == 'SEA'\") \\\n",
    "                        .filter(\"DESTINATION_AIRPORT == 'ANC'\")\n",
    "filter_SEA_ANC.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762d86b",
   "metadata": {},
   "source": [
    ".groupBy(“column1”, “column2”,…,”columnt”): Tương tự như lệnh GROUP BY của SQL, lệnh này sẽ nhóm các biến theo các dimension được truyền vào groupBy. Theo sau lệnh groupBy() là một build-in function của spark DataFrame được sử dụng để tính toán theo một biến đo lường nào đó chẳng hạn như hàm avg(), min(), max(), sum(). Tham số được truyền vào các hàm này chính là tên biến đo lường.\n",
    "\n",
    "#### Bên dưới chúng ta sẽ tính thời gian bay trung bình theo điểm xuất phát (ORIGIN_AIRPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70dd6b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|ORIGIN_AIRPORT|     avg(HOUR_ARR)|\n",
      "+--------------+------------------+\n",
      "|           BGM| 1.096525096525096|\n",
      "|           PSE|3.0352529358626916|\n",
      "|           INL|0.5327937649880096|\n",
      "|           DLG|0.8359307359307364|\n",
      "|         12888|0.4413461538461539|\n",
      "+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_time_org_airport = flights.groupBy(\"ORIGIN_AIRPORT\").avg(\"HOUR_ARR\")\n",
    "avg_time_org_airport.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0d767",
   "metadata": {},
   "source": [
    "### 2.5. các lệnh biến đổi dữ liệu của spark.sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570f8e7",
   "metadata": {},
   "source": [
    "SELECT: Có cú pháp chung là: SELECT * FROM (TABLENAME) WHERE (CONDTION) Lệnh này sẽ lựa chọn các trường trong bảng theo điều kiện tại WHERE. \n",
    "        \n",
    "#### Chẳng hạn bên dưới chúng ta lấy ra những chuyến bay có thời gian bay > 10 phút."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6ea9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|    1|  1|          4|     AS|           98|     N407AS|           ANC|                SEA|               0005|          2354|            -11|      21|      0015|           205|         194|     169|    1448|     0404|      4|             0430|        0408|          -22|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|         2336|     N3KUAA|           LAX|                PBI|               0010|          0002|             -8|      12|      0014|           280|         279|     263|    2330|     0737|      4|             0750|        0741|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     US|          840|     N171US|           SFO|                CLT|               0020|          0018|             -2|      16|      0034|           286|         293|     266|    2296|     0800|     11|             0806|        0811|            5|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|          258|     N3HYAA|           LAX|                MIA|               0020|          0015|             -5|      15|      0030|           285|         281|     258|    2342|     0748|      8|             0805|        0756|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AS|          135|     N527AS|           SEA|                ANC|               0025|          0024|             -1|      11|      0035|           235|         215|     199|    1448|     0254|      5|             0320|        0259|          -21|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_10 = my_spark.sql('SELECT * FROM flights_temp WHERE AIR_TIME > 10')\n",
    "flights_10.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ff1fb",
   "metadata": {},
   "source": [
    "GROUP BY: Lệnh này sẽ thống kê các các trường measurement (các trường dùng để tính toán) theo một nhóm các trường dimension (các trường dùng để phân loại) dựa trên các aggregation function như sum(), avg(), min(), max(), mean(), median(), count(), count(distinct()) của SQL.\n",
    "    \n",
    "#### Chẳng hạn chúng ta muốn tính số phút bay trung bình của mỗi hãng bay trong năm sẽ sử dụng hàm GROUP BY như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e1b1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='flights_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "+--------------+-------------------+-----------+------------------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|TAIL_NUMBER|         avg_speed|\n",
      "+--------------+-------------------+-----------+------------------+\n",
      "|           IAG|                FLL|     N630NK|             158.0|\n",
      "|           RIC|                ATL|     N947DN|  75.6470588235294|\n",
      "|           EWR|                ATL|     N970AT|108.02777777777777|\n",
      "|           MSN|                ORD|     N703SK|              28.0|\n",
      "|           AVL|                ATL|     N994AT|              30.5|\n",
      "+--------------+-------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(my_spark.catalog.listTables())\n",
    "agg_arr_time = my_spark.sql(\"SELECT ORIGIN_AIRPORT, DESTINATION_AIRPORT, TAIL_NUMBER, MEAN(AIR_TIME) AS avg_speed FROM flights_temp GROUP BY ORIGIN_AIRPORT, DESTINATION_AIRPORT, TAIL_NUMBER\")\n",
    "agg_arr_time.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df8f82",
   "metadata": {},
   "source": [
    "## 3. Xây dựng pipeline End-to-End model trên pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65410c",
   "metadata": {},
   "source": [
    "### 3.1. Xây dựng pipeline biến đổi dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073422a7",
   "metadata": {},
   "source": [
    "pyspark cho phép xây dựng các end-to-end model mà dữ liệu truyền vào là các raw data và kết quả trả ra là nhãn, xác xuất hoặc giá trị được dự báo của model. Các end-to-end model này được đi qua một pipe line của\n",
    "\n",
    "pyspark.ml bao gồm 2 class cơ bản là Transformer cho phép biến đổi dữ liệu và Estimator ước lượng mô hình dự báo.\n",
    "\n",
    "Transfromer sử dụng hàm .transform() nhận đầu vào là 1 DataFrame và trả ra một DataFrame mới có các trường đã biến đổi theo Transform. \n",
    "\n",
    "Estimator sử dụng hàm .fit() để huấn luyện model. Chúng cũng nhận đầu vào là một DataFrame nhưng kết quả được trả ở đầu ra là 1 model object. Hiện tại spark hỗ trợ khá nhiều các lớp model cơ bản trong machine learning. Các lớp model xuất hiện trong Esimator bao gồm:\n",
    "\n",
    "Đối với bài toán phân loại: LogisticRegression, DecisionTreeClassifier, RandomForestModel, GBTClassifier (gradient bosting tree), MultilayerPerceptronClassifier, LinearSVC (Linear Support Vector Machine), NaiveBayes.\n",
    "\n",
    "Đối với bài toán dự báo: GeneralizedLinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor (gradient boosting Tree), AFTSurvivalRegression (Hồi qui đối với các lớp bài toán estimate survival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de0a0b",
   "metadata": {},
   "source": [
    "#### Xây dựng 1 pipeline cho model dự báo khả năng trễ chuyến bay dựa trên dữ liệu đầu vào là bảng flights\n",
    "Tìm hiểu các trường trong bảng dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3610dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: string (nullable = true)\n",
      " |-- MONTH: string (nullable = true)\n",
      " |-- DAY: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: string (nullable = true)\n",
      " |-- DEPARTURE_TIME: string (nullable = true)\n",
      " |-- DEPARTURE_DELAY: string (nullable = true)\n",
      " |-- TAXI_OUT: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- SCHEDULED_TIME: string (nullable = true)\n",
      " |-- ELAPSED_TIME: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      " |-- TAXI_IN: string (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: string (nullable = true)\n",
      " |-- ARRIVAL_TIME: string (nullable = true)\n",
      " |-- ARRIVAL_DELAY: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- AIRLINE_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      " |-- HOUR_ARR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38166429",
   "metadata": {},
   "source": [
    "#### *Bài toán:* \n",
    "Lấy dữ liệu của những chuyến bay xuất phát từ ‘SEA’ của hãng delta airline và american airlines (‘DA’ và ‘AA’). Dữ liệu dự báo bao gồm các trường: ARRIVAL_DELAY, ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK, DESTINATION_AIRPORT, AIRLINE. Trong đó trường ARRIVAL_DELAY > 0 xác định chuyến bay trễ và ARRIVAL_DELAY = 0 chuyến bay không bị trễ.\n",
    "\n",
    "Do các mô hình chỉ nhận đầu vào là các biến numeric nên ta phải có các bước xử lý dữ liệu từ biến string, boolean sang biến numeric.\n",
    "\n",
    "Chúng ta sẽ phân các biến trên thành 2 nhóm biến là:\n",
    "\n",
    "Các biến numeric: ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK. Không cần phải qua biến đổi và được sử dụng trực tiếp làm đầu vào của model hồi qui. Tuy nhiên các biến này đang được để ở dạng string nên phải chuyển qua numeric bằng hàm CAST.\n",
    "\n",
    "Các biến string: DESTINATION_AIRPORT, AIRLINE là các biến dạng category cần được đánh index và biến đổi sang dạng biến dummies (chỉ nhận giá trị 0 và 1) để có thể đưa vào model hồi qui. Khi đó mỗi một biến sẽ được phân thành nhiều features mà mỗi một features đại diện cho 1 nhóm của biến. Quá trình biến đổi dummies sẽ trải qua 2 bước: Đánh index cho biến bằng class StringIndexer(). Một index sẽ được gán cho 1 nhóm biến. Biểu diễn one-hot vector thông qua class OneHotEncoder(): Từ index của biến sẽ biểu diễn các biến dưới dạng one-hot vector sao cho vị trí bằng 1 sẽ là phần tử có thứ tự trùng với index. Cả 2 class StringIndexer() và OneHotEncoder() đều là các object của pyspark.ml.feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e1df2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of previous data: (5819079, 32)\n",
      "Shape of flights_SEA data: (19956, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of previous data: ({}, {})'.format(flights.count(), len(flights.columns)))\n",
    "flights_SEA = my_spark.sql(\"select ARRIVAL_DELAY, ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK, DESTINATION_AIRPORT, AIRLINE from flights_temp where ORIGIN_AIRPORT = 'SEA' and AIRLINE in ('DL', 'AA') \")\n",
    "print('Shape of flights_SEA data: ({}, {})'.format(flights_SEA.count(), len(flights_SEA.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0990d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of model_data data: (19823, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create boolean variable IS_DELAY variable as Target\n",
    "flights_SEA = flights_SEA.withColumn(\"IS_DELAY\", flights_SEA.ARRIVAL_DELAY > 0)\n",
    "# Now Convert Boolean variable into integer\n",
    "flights_SEA = flights_SEA.withColumn(\"label\", flights_SEA.IS_DELAY.cast(\"integer\"))\n",
    "# Remove missing value\n",
    "model_data = flights_SEA.filter(\"ARRIVAL_DELAY is not null \\\n",
    "                                and ARRIVAL_TIME is not null \\\n",
    "                                and MONTH is not null \\\n",
    "                                and YEAR is not null  \\\n",
    "                                and DAY_OF_WEEK is not null \\\n",
    "                                and DESTINATION_AIRPORT is not null \\\n",
    "                                and AIRLINE is not null\")\n",
    "\n",
    "print('Shape of model_data data: ({}, {})'.format(model_data.count(), len(model_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021b4f4",
   "metadata": {},
   "source": [
    "Các model phân loại của pyspark luôn mặc định nhận biến dự báo là label. Do đó trong bất kì model nào chúng ta cũng cần tạo ra biến integer là nhãn của model dưới tên label.\n",
    "\n",
    "Convert các biến string sang numeric bằng hàm .withColumn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05b07884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ARRIVAL_DELAY: string (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- IS_DELAY: boolean (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ARRIVAL_TIME, MONTH, YEAR, DAY_OF_WEEK\n",
    "model_data = model_data.withColumn(\"ARRIVAL_TIME\", model_data.ARRIVAL_TIME.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"MONTH\", model_data.MONTH.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"YEAR\", model_data.YEAR.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"DAY_OF_WEEK\", model_data.DAY_OF_WEEK.cast(\"integer\"))\n",
    "model_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0de2ad",
   "metadata": {},
   "source": [
    "Biến đổi các biến String bằng StringIndexer và OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2710d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "# I. With DESTINATION_AIRPORT\n",
    "# Create StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol = \"DESTINATION_AIRPORT\", \\\n",
    "                             outputCol = \"DESTINATION_INDEX\")\n",
    "\n",
    "# Create OneHotEncoder\n",
    "dest_onehot = OneHotEncoder(inputCol = \"DESTINATION_INDEX\", \\\n",
    "                            outputCol = \"DESTINATION_FACT\")\n",
    "\n",
    "# II. With AIRLINE\n",
    "# Create StringIndexer\n",
    "airline_indexer = StringIndexer(inputCol = \"AIRLINE\", \\\n",
    "                                outputCol = \"AIRLINE_INDEX\")\n",
    "\n",
    "# Create OneHotEncoder\n",
    "airline_onehot = OneHotEncoder(inputCol = \"AIRLINE_INDEX\", \\\n",
    "                               outputCol = \"AIRLINE_FACT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a6930",
   "metadata": {},
   "source": [
    "Đầu ra của quá trình biến đổi trên 2 biến DESTINATION_AIRPORT và AIRLINE chính là biến DESTINATION_FACT và AIRLINE_FACT. Hai biến này sẽ cần được đưa vào vector tổng hợp sẽ được giới thiệu biên dưới. Các objects: dest_indexer, dest_onehot, airline_indexer, airline_onehot sẽ được truyền vào pipeline theo thứ tự để thể hiện quá trình transform dữ liệu tuần tự từ trái qua phải.\n",
    "\n",
    "Bước cuối cùng của pipeline là kết hợp toàn bộ các columns chứa các features thành một column duy nhất. Bước này phải được thực hiện trước khi model training bởi spark model sẽ chỉ chấp nhận đầu vào ở định dạng này. Chúng ta có thể lưu mỗi một giá trị từ một cột như một phần tử của vector. Khi đó vector sẽ chứa toàn bộ các thông tin cần thiết của 1 quan sát để xác định nhãn hoặc giá trị dự báo ở đầu ra của quan sát đó. Class VectorAssembler của pyspark.ml.feature sẽ tạo ra vector tổng hợp đại diện cho toàn bộ các chiều của quan sát đầu vào. Việc chúng ta cần thực hiện chỉ là truyền vào class list string tên các trường thành phần của vector tổng hợp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dfce6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols = [\"ARRIVAL_TIME\", \"MONTH\", \"YEAR\", \\\n",
    "                                             \"DAY_OF_WEEK\", \"DESTINATION_FACT\",\\\n",
    "                                             \"AIRLINE_FACT\"], \n",
    "                                outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39236cd",
   "metadata": {},
   "source": [
    "Sau stage này chúng ta sẽ tổng hợp các biến trong inputCols thành một vector dự báo ở outputCol được lưu dưới tên features. Nhãn của model luôn mặc định là biến label đã khởi tạo từ đầu.\n",
    "\n",
    "Tiếp theo chúng ta sẽ khởi tạo pipeline biến đổi dữ liệu cho model thông qua class Pipeline của pyspark.ml. Các transformer biến đổi dữ liệu sẽ được sắp xếp trong 1 list và truyền vào tham số stages như bên dưới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2855ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Make a pipeline\n",
    "flights_sea_pipe  = Pipeline(stages = [dest_indexer, dest_onehot, airline_indexer, \\\n",
    "                                       airline_onehot, vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c3b2c",
   "metadata": {},
   "source": [
    "### 3.2. Huấn luyện và đánh giá model\n",
    "#### 3.2.1. Phân chia tập train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd55245",
   "metadata": {},
   "source": [
    "Sau khi đi qua pipe line ở bước Transform dữ liệu chúng ta sẽ thu được vector tổng hợp (Vector Assemble) và nhãn (LABLE) ở đầu ra. Tiếp theo tại bước này sẽ thực hiện phân chia tập train/test theo tỷ lệ 80%/20%. Trong đó model được xây dựng trên tập Train và được đánh giá trên tập Test. Tại sao lại phải để riêng 1 tập test mà không hồi qui trên toàn bộ dữ liệu? Đó là vì model thường phân loại hoặc dự báo tốt trên dữ liệu nó đã được huấn luyện mà không phân loại, dự báo tốt đối với dữ liệu mới. Chính vì thế tập test được coi như dữ liệu mới không được sử dụng trong huấn luyện và dùng để đánh giá khả năng dự báo của model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af327b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipe_data from pipeline\n",
    "pipe_data = flights_sea_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aaa1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test data\n",
    "train, test = pipe_data.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f490cc",
   "metadata": {},
   "source": [
    "#### 3.2.2. Huấn luyện và đánh giá model\n",
    "Có rất nhiều các lớp model phân loại đã được giới thiệu trong mục 3.1. Để đơn giản hóa chúng ta sẽ chọn ra model LogisticRegression trong ví dụ demo này làm model phân loại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05a368c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create logistic regression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4ed05",
   "metadata": {},
   "source": [
    "Để đánh giá model chúng ta cần sử dụng các metric như ROC, Accuracy, F1, precision hoặc recal. Do dữ liệu không có hiện tượng mất cân bằng giữa 2 lớp nên ta sẽ sử dụng ROC làm metric đánh giá model. Trong trường hợp mẫu mất cân bằng thì các chỉ số F1, precision hoặc recal nên được sử dụng thay thế vì trong tình huống này ROC, Accuracy thường mặc định là rất cao. Chúng ta sử dụng class BinaryClassificationEvaluator trong pyspark.ml.evaluation module để tính toán các metrics đánh giá model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "289755d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName = \"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7d2ef",
   "metadata": {},
   "source": [
    "#### 3.2.3. Tuning model thông qua param gridSearch.\n",
    "\n",
    "Pyspark cho phép chúng ta tuning model trên một gridSearch các params. Tuning model được hiểu là tìm kiếm trong 1 không gian các tham số của model để thu được model mà giá trị của metric khai báo ở bước 3.2.2 thu được là nhỏ nhất trên tập test. \n",
    "\n",
    "Để xây dựng một gridSearch chúng ta sử dụng class ParamGridBuilder của submodule pyspark.ml.tuning. Để thêm các gridSearch chúng ta sử dụng phương thức .addGrid() và sau đó sử dụng hàm .build() để khởi tạo gridSearch. Mỗi lớp model sẽ có các tham số khác nhau. \n",
    "\n",
    "Do đó để tuning được model cần phải xác định xem model có những tham số gì. Việc này đòi hỏi bạn phải nắm vững về thuật toán và phương pháp tối ưu của model. Mình sẽ không đi sâu vào phần này. Đối với LogisticRegression mình sẽ lựa chọn lr.regParam là tham số tuning. Một lưu ý khác là việc tuning có thể rất tốt thời gian và tài nguyên của máy tính do thực hiện nhiều model của cùng 1 lớp model nhưng với các tham số khau. Do đó bạn cần cân nhắc cấu hình máy trước khi thực hiện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3184b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "import numpy as np\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter, we can add more than one hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, 0.1, 0.01))\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7ae73",
   "metadata": {},
   "source": [
    "Sau khi build xong gridSearch chúng ta cần Cross Validate toàn bộ các model trên tập các tham số khởi tạo ở grid. Chúng ta cần khởi tạo một cross validator từ class CrossValidator của pyspark.ml.tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d195abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )\n",
    "\n",
    "# Fit cross validation on models\n",
    "models = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb0be7",
   "metadata": {},
   "source": [
    "Sau khi tuning xong chúng ta sẽ thu được model tốt nhất thông qua hàm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01eb812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = models.bestModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc29cac",
   "metadata": {},
   "source": [
    "Kiểm tra mức độ chính xác của model trên tập test bằng hàm evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "557bb279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5959475419125124\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f2aec",
   "metadata": {},
   "source": [
    "Như vậy trên tập test model đạt mức độ chính xác khoảng 59.6%. Phương án tốt được cân nhắc là thêm biến để cải thiện model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b417a6",
   "metadata": {},
   "source": [
    "# Tổng kết\n",
    "Như vậy thông qua bài viết này chúng ta đã được nắm bắt được hiệu quả của spark trong xử lý dữ liệu lớn. Tổng hợp lại các kiến thức thu được trong bài:\n",
    "\n",
    "1.Cách thức khởi tạo một spark context và quản lý các table trên catalog của cluster.\n",
    "\n",
    "2.Các lệnh cơ bản biến đổi và tổng hợp dữ liệu trên spark DataFrame.\n",
    "\n",
    "3.Các lệnh SQL được tích hợp trên spark.\n",
    "\n",
    "4.Tạo pipeline biến đổi dữ liệu cho model phân loại.\n",
    "\n",
    "5.Xây dựng và đánh giá model trên pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e348352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
